compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
library(keras)
library(ISLR)
Hitters <- na.omit(Hitters)
Hitters$LogSalary <- log(Hitters$Salary)
Hitters$Salary <- NULL
n <- nrow(Hitters)
p <- ncol(Hitters) - 1
set.seed(1)
train <- sample(n, 0.8*n)
#Training Data/labels and Test data/labels
train_data <- Hitters[train, -1]
train_labels <- Hitters[train, 1]
test_data <- Hitters[-train, -1]
test_labels <- Hitters[-train, 1]
#Listing the names needed for OHE and perform OHE on them
summary(Hitters)
train_data$League <- to_categorical(as.numeric(train_data$League)-1)
head(train_data$League)
train_data$Division <- to_categorical(as.numeric(train_data$Division)-1)
head(train_data$Division)
train_data$NewLeague <- to_categorical(as.numeric(train_data$NewLeague) -1)
head(train_data$NewLeague)
test_data$League <- to_categorical(as.numeric(test_data$League)-1)
head(test_data$League)
test_data$Division <- to_categorical(as.numeric(test_data$Division)-1)
head(test_data$Division)
test_data$NewLeague <- to_categorical(as.numeric(test_data$NewLeague) -1)
head(test_data$NewLeague)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data,
center = col_means_train,
scale = col_stddevs_train)
#Creating the Model
model <- keras_model_sequential(layers=list(
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]),
layer_dense(units = 1)
))
#Compiling the Model
compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
library(keras)
library(ISLR)
Hitters <- na.omit(Hitters)
Hitters$LogSalary <- log(Hitters$Salary)
Hitters$Salary <- NULL
n <- nrow(Hitters)
p <- ncol(Hitters) - 1
set.seed(1)
train <- sample(n, 0.8*n)
#Training Data/labels and Test data/labels
train_data <- Hitters[train, -1]
train_labels <- Hitters[train, 1]
test_data <- Hitters[-train, -1]
test_labels <- Hitters[-train, 1]
#Listing the names needed for OHE and perform OHE on them
summary(Hitters)
train_data$League <- to_categorical(as.numeric(train_data$League)-1)
head(train_data$League)
train_data$Division <- to_categorical(as.numeric(train_data$Division)-1)
head(train_data$Division)
train_data$NewLeague <- to_categorical(as.numeric(train_data$NewLeague) -1)
head(train_data$NewLeague)
test_data$League <- to_categorical(as.numeric(test_data$League)-1)
head(test_data$League)
test_data$Division <- to_categorical(as.numeric(test_data$Division)-1)
head(test_data$Division)
test_data$NewLeague <- to_categorical(as.numeric(test_data$NewLeague) -1)
head(test_data$NewLeague)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data,
center = col_means_train,
scale = col_stddevs_train)
#Creating the Model
model <- keras_model_sequential(layers=list(
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]),
layer_dense(units = 1)
))
#Compiling the Model
compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
library(keras)
library(ISLR)
Hitters <- na.omit(Hitters)
Hitters$LogSalary <- log(Hitters$Salary)
Hitters$Salary <- NULL
n <- nrow(Hitters)
p <- ncol(Hitters) - 1
set.seed(1)
train <- sample(n, 0.8*n)
#Training Data/labels and Test data/labels
train_data <- Hitters[train, -1]
train_labels <- Hitters[train, 1]
test_data <- Hitters[-train, -1]
test_labels <- Hitters[-train, 1]
#Listing the names needed for OHE and perform OHE on them
summary(Hitters)
train_data$League <- to_categorical(as.numeric(train_data$League)-1)
head(train_data$League)
train_data$Division <- to_categorical(as.numeric(train_data$Division)-1)
head(train_data$Division)
train_data$NewLeague <- to_categorical(as.numeric(train_data$NewLeague) -1)
head(train_data$NewLeague)
test_data$League <- to_categorical(as.numeric(test_data$League)-1)
head(test_data$League)
test_data$Division <- to_categorical(as.numeric(test_data$Division)-1)
head(test_data$Division)
test_data$NewLeague <- to_categorical(as.numeric(test_data$NewLeague) -1)
head(test_data$NewLeague)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data,
center = col_means_train,
scale = col_stddevs_train)
#Creating the Model
model <- keras_model_sequential(layers=list(
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]),
layer_dense(units = 1)
))
#Compiling the Model
compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
library(keras)
library(ISLR)
Hitters <- na.omit(Hitters)
Hitters$LogSalary <- log(Hitters$Salary)
Hitters$Salary <- NULL
n <- nrow(Hitters)
p <- ncol(Hitters) - 1
set.seed(1)
train <- sample(n, 0.8*n)
#Training Data/labels and Test data/labels
train_data <- Hitters[train, -1]
train_labels <- Hitters[train, 1]
test_data <- Hitters[-train, -1]
test_labels <- Hitters[-train, 1]
#Listing the names needed for OHE and perform OHE on them
summary(Hitters)
train_data$League <- to_categorical(as.numeric(train_data$League)-1)
head(train_data$League)
train_data$Division <- to_categorical(as.numeric(train_data$Division)-1)
head(train_data$Division)
train_data$NewLeague <- to_categorical(as.numeric(train_data$NewLeague) -1)
head(train_data$NewLeague)
test_data$League <- to_categorical(as.numeric(test_data$League)-1)
head(test_data$League)
test_data$Division <- to_categorical(as.numeric(test_data$Division)-1)
head(test_data$Division)
test_data$NewLeague <- to_categorical(as.numeric(test_data$NewLeague) -1)
head(test_data$NewLeague)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data,
center = col_means_train,
scale = col_stddevs_train)
#Creating the Model
model <- keras_model_sequential(layers=list(
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]),
layer_dense(units = 1)
))
#Compiling the Model
compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
##Homework 6 Problem 5
library(keras)
library(ISLR)
Hitters <- na.omit(Hitters)
Hitters$LogSalary <- log(Hitters$Salary)
Hitters$Salary <- NULL
n <- nrow(Hitters)
p <- ncol(Hitters)-1
set.seed(1)
train <- sample(n, 0.8*n)
#Training Data/labels and Test data/labels
train_data <- Hitters[train, -1]
train_labels <- Hitters[train, 1]
test_data <- Hitters[-train, -1]
test_labels <- Hitters[-train, 1]
#Listing the names needed for OHE and perform OHE on them
summary(Hitters)
train_data$League <- to_categorical(as.numeric(train_data$League)-1)
head(train_data$League)
train_data$Division <- to_categorical(as.numeric(train_data$Division)-1)
head(train_data$Division)
train_data$NewLeague <- to_categorical(as.numeric(train_data$NewLeague) -1)
head(train_data$NewLeague)
test_data$League <- to_categorical(as.numeric(test_data$League)-1)
head(test_data$League)
test_data$Division <- to_categorical(as.numeric(test_data$Division)-1)
head(test_data$Division)
test_data$NewLeague <- to_categorical(as.numeric(test_data$NewLeague) -1)
head(test_data$NewLeague)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data,
center = col_means_train,
scale = col_stddevs_train)
#Creating the Model
model <- keras_model_sequential(layers=list(
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]),
layer_dense(units = 1)
))
#Compiling the Model
compile(model,
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
model
#Training the Model
#Displaying the training progress
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs){
if(epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- fit(model, train_data, train_labels,
epochs = 300, batch_size = 20,
validation_split = 0.2, verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth=F)
#ANN Test Prediction Performance
test_predictions <- predict(model, test_data)
test_predictions[ ,1]
head(data.frame(true = test_labels, predicted = test_predictions))
eval.results <- evaluate(model, test_data, test_labels, verbose = 0)
mae <- eval.results$mean_absolute_error
mae
Hitters$LogSalary <- log(Hitters$Salary)
#Problem 4
activation_function_neuron <- function(input, weights, bias, act)
{
z = sum(c(input)*c(weights)) + bias
if(act == "binary")
{
if(z > 0) z = 1
else if (z <= 0) z = 0
return(z)
}
else if(act == "relu")
{
if(z <= 0) z = 0
return(z)
}
else if(act == "sigmoid")
{
return(1/(1+exp(-(z))))
}
}
#2b
bias = 1
weights = c(0.2, -0.5, 0.5, -0.4, 0.2)
inputs = c(-5, 2, 4, -10, 10)
activation_function_neuron(inputs, weights, bias, "binary")
activation_function_neuron(inputs, weights, bias, "relu")
activation_function_neuron(inputs, weights, bias, "sigmoid")
##Converting Cross-Section to Pixsets
##Load relevant Libraries
library(magick)
library(imager)
library(EBImage)
#Reading and Writing Image
inp_img2 <- readImage("C:/Users/USER/Desktop/CSPNG/slice_1.png")
str(inp_img2)
dim(inp_img2)
#COnverting image to 2d matrix of pixels
is.Image(as.array(inp_img2))
inp_mat2 <- imageData(inp_img2)[1:480, 1:480, 1:3]
inp_mat2_new <- inp_mat2[-3]
inp_mat2_new/255
inp_img2 <- readImage("C:/Users/USER/Desktop/CSPNG/slice_1.png")
library(magick)
library(imager)
library(EBImage)
##Converting Cross-Section to Pixsets
##Load relevant Libraries
library(magick)
library(imager)
library(EBImage)
#Reading and Writing Image
inp_img2 <- readImage("C:/Users/USER/Desktop/CSPNG/slice_1.png")
str(inp_img2)
dim(inp_img2)
#COnverting image to 2d matrix of pixels
is.Image(as.array(inp_img2))
inp_mat2 <- imageData(inp_img2)[1:480, 1:480, 1:3]
inp_mat2_new <- inp_mat2[-3]
inp_mat2_new/255
install.packages("magick")
install.packages("imager")
install.packages("EBImage")
install.packages("BiocManager")
BiocManager::install("EBImage")
library(magick)
library(imager)
library(EBImage)
inp_img2 <- readImage("C:/Users/USER/Desktop/CSPNG/slice_1.png")
str(inp_img2)
dim(inp_img2)
is.Image(as.array(inp_img2))
inp_mat2 <- imageData(inp_img2)[1:480, 1:480, 1:3]
inp_mat2_new <- inp_mat2[-3]
inp_mat2 <- imageData(inp_img2)[1:480, 1:480, 1:3]
inp_mat2_new <- inp_mat2[-3]
inp_mat2_new/255
inp_mat2_new
summary(inp_mat2_new)
library(magick)
library(imager)
library(EBImage)
#Reading and Writing Image
inp_img1 <- load.image("D:/Users/Public/Public Documents/CStoPNG/slice_1.png")
str(inp_img1)
dim(inp_img1)
plot(inp_img1)
install.packages(imager)
install.packages("imager")
library(imager)
install.packages("EBImage")
insta..packages("BiocManager")
install.packages("BiocManager")
BiocManager::install("EBImage")
library(iamger)
library(imager)
install.packages("magick")
##Converting Cross-Section to Pixsets
##Load relevant Libraries
library(imager)
library(EBImage)
#Reading and Writing Image
inp_img1 <- load.image("C:/Users/young/OneDrive/Desktop/PNG Cross Sections/slice_1.png")
#Converting image to 2d matrix of pixels
inp_mat1 <- matrix (data = inp_img1, ncol = 480, nrow = 480, byrow = FALSE)
inp_mat1 <- inp_mat1[nrow(inp_mat1):1, ]
#Function to subset a matrix into x by y matrices.
matsplitter <- function (M, r, c)
{
simplify2array (lapply (split (M, interaction ((row (M) - 1) %/% r + 1,(col (M) - 1) %/% c + 1)),
function(x) {dim(x) <- c(r,c); x;}))
}
pixbox_mat <- matsplitter(inp_mat1, 24, 24) ##Created 400 24 by 24 matrices
#Initializing For loop Variables
setwd ("C:/Users/young/OneDrive/Desktop/Noise Free Sections")
blackcounter = 0 #To see how many black pixels are in each matrix
#For loop.
#We are trying to make this output the matrixes that have more than 15 black pixels
for (i in 1:400)
{
for (j in 1:24)
{
for (k in 1:24)
{
if (pixbox_mat[k,j,i] == 0)
{
blackcounter = blackcounter + 1
}
}
}
if (blackcounter > 15)
{
png (paste0 ("subsection_", i, ".png", sep = ""))
par (mar = c(0,0,0,0))
image  (pixbox_mat[ , , i], axes = FALSE, col = grey (seq (0, 1, length = 256)))
dev.off()
}
blackcounter = 0
}
##OUTPUTS FLIPPED IMAGES THOUGH
